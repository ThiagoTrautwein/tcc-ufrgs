{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCC - An√°lise de Sinais EEG\n",
    "\n",
    "Este notebook apresenta um fluxo de trabalho completo para a an√°lise de sinais de EEG, desde a leitura e pr√©-processamento dos dados at√© a constru√ß√£o, treinamento e avalia√ß√£o de um modelo de aprendizado profundo para classifica√ß√£o."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bibliotecas e Configura√ß√µes Iniciais\n",
    "\n",
    "Importa√ß√£o das bibliotecas necess√°rias e configura√ß√£o do ambiente, incluindo a aloca√ß√£o de mem√≥ria da GPU, se dispon√≠vel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rclone mount drive-thiago: ~/gdrive --daemon\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, Bidirectional, LSTM, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Configura√ß√£o da GPU (opcional)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4000)]\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Caminhos dos arquivos\n",
    "DATALAKE_DIR = '/home/thiago/gdrive/Acad√™mico/UFRGS/tcc-ufrgs/datalake'\n",
    "RAW_MUSE_PATH = f'{DATALAKE_DIR}/raw/Muse-v1.0/MU.txt'\n",
    "INDEX = 10_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Leitura dos Dados\n",
    "\n",
    "Leitura do arquivo bruto e convers√£o dos dados em um formato adequado para o treinamento do modelo de deep learning. Cada amostra √© convertida em um array NumPy com shape `(n_amostras, TARGET_LEN, n_canais)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    MAX_LEN = 440\n",
    "\n",
    "    df = pd.read_csv(RAW_MUSE_PATH, sep='\\t', header=None, names=[\"id\", \"event\", \"device\", \"channel\", \"code\", \"size\", \"data\"])\n",
    "\n",
    "    df = df[df['code'] != -1]\n",
    "\n",
    "    df[\"data_array\"] = df[\"data\"].apply(lambda x: np.array([int(v) for v in x.split(\",\")]))\n",
    "\n",
    "    df = df.sort_values([\"event\", \"channel\"]).reset_index(drop=True)\n",
    "\n",
    "    def pad_or_truncate(arr):\n",
    "        if len(arr) > MAX_LEN:\n",
    "            return arr[:MAX_LEN]\n",
    "        elif len(arr) < MAX_LEN:\n",
    "            return np.pad(arr, (0, MAX_LEN - len(arr)), mode='constant')\n",
    "        return arr\n",
    "\n",
    "    df[\"data_array\"] = df[\"data_array\"].apply(pad_or_truncate)\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for event, group in df.groupby(\"event\"):\n",
    "        channel_arrays = [arr for arr in group[\"data_array\"].to_list()]\n",
    "        sample = np.stack(channel_arrays, axis=-1)  # (timesteps, n_channels)\n",
    "        X.append(sample)\n",
    "        y.append(group[\"code\"].iloc[0])\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    print(\"Shape final de X:\", X.shape)\n",
    "    print(\"Shape final de y:\", y.shape)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X, y = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pr√©-processamento e Filtragem dos Sinais\n",
    "\n",
    "Aplica√ß√£o de filtros para remover ru√≠dos e artefatos dos sinais de EEG. As seguintes t√©cnicas s√£o utilizadas:\n",
    "- **Filtro Butterworth Passa-Alta:** Para remover a flutua√ß√£o da linha de base.\n",
    "- **Filtro Notch:** Para remover a interfer√™ncia da rede el√©trica (60 Hz).\n",
    "- **Denoising com Transformada Wavelet Discreta (DWT):** Para atenuar ru√≠dos de alta frequ√™ncia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import butter, filtfilt, iirnotch\n",
    "import pywt\n",
    "\n",
    "class Preprocessing:\n",
    "    def __init__(self, fs=220):\n",
    "        self.fs = fs\n",
    "\n",
    "    def butterworth_highpass(self, data, cutoff=0.1, order=5):\n",
    "        b, a = butter(order, cutoff / (self.fs / 2), btype=\"high\", analog=False)\n",
    "        return filtfilt(b, a, data)\n",
    "\n",
    "    def notch_filter(self, data, freq=60.0, Q=30.0):\n",
    "        b, a = iirnotch(w0=freq/(self.fs/2), Q=Q)\n",
    "        return filtfilt(b, a, data)\n",
    "\n",
    "    def dwt_denoise_reconstruct(self, signal, wavelet='db4', level=3, mode='soft'):\n",
    "        coeffs = pywt.wavedec(signal, wavelet=wavelet, level=level)\n",
    "        n = len(signal)\n",
    "        for i in range(1, len(coeffs)):\n",
    "            cd = coeffs[i]\n",
    "            sigma = np.median(np.abs(cd)) / 0.6745 if cd.size > 0 else 0.0\n",
    "            thresh = sigma * np.sqrt(2 * np.log(n)) if sigma > 0 else 0.0\n",
    "            coeffs[i] = pywt.threshold(cd, thresh, mode=mode)\n",
    "        rec = pywt.waverec(coeffs, wavelet=wavelet)\n",
    "        return np.asarray(rec[:n])\n",
    "    \n",
    "    def _save_sample(self, X, y, etapa):\n",
    "        df_sample = pd.DataFrame(X, columns=[f\"canal_{i+1}\" for i in range(X.shape[1])])\n",
    "        df_sample[\"label\"] = y\n",
    "        df_sample.to_csv(f\"sample_{etapa}.csv\", index=False)\n",
    "\n",
    "    def execute(self, X, y):\n",
    "        n_samples, timesteps, n_channels = X.shape\n",
    "        print(f\"üîπ Iniciando pr√©-processamento: {n_samples} amostras, {n_channels} canais, {timesteps} timesteps\")\n",
    "\n",
    "        # Arrays intermedi√°rios\n",
    "        X_raw = np.zeros((1, 440, 4), dtype=float)\n",
    "        X_highpass = np.zeros((1, 440, 4), dtype=float)\n",
    "        X_notch = np.zeros((1, 440, 4), dtype=float)\n",
    "        X_dwt = np.zeros((1, 440, 4), dtype=float)\n",
    "        X_filtered = np.zeros_like(X, dtype=float)\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            for ch in range(n_channels):\n",
    "                signal = X[i, :, ch].astype(float)\n",
    "                if i == INDEX:\n",
    "                    X_raw[0, :, ch] = signal\n",
    "\n",
    "                # Etapa 1: High-pass\n",
    "                signal = self.butterworth_highpass(signal, cutoff=0.1, order=5)\n",
    "                if i == INDEX:\n",
    "                    X_highpass[0, :, ch] = signal\n",
    "\n",
    "                # Etapa 2: Notch\n",
    "                signal = self.notch_filter(signal, freq=60.0, Q=30.0)\n",
    "                if i == INDEX:\n",
    "                    X_notch[0, :, ch] = signal\n",
    "\n",
    "                # Etapa 3: DWT\n",
    "                signal = self.dwt_denoise_reconstruct(signal, wavelet='db4', level=3, mode='soft')\n",
    "                if i == INDEX:\n",
    "                    X_dwt[0, :, ch] = signal\n",
    "\n",
    "                X_filtered[i, :, ch] = signal\n",
    "        \n",
    "        print(X_highpass.shape, X_notch.shape, X_dwt.shape)\n",
    "        self._save_sample(X_raw[0], y[INDEX], \"raw\")\n",
    "        self._save_sample(X_highpass[0], y[INDEX], \"highpass\")\n",
    "        self._save_sample(X_notch[0], y[INDEX], \"notch\")\n",
    "        self._save_sample(X_dwt[0], y[INDEX], \"dwt\")\n",
    "\n",
    "        print(\"‚úÖ Pr√©-processamento conclu√≠do.\")\n",
    "        return X_filtered, y\n",
    "\n",
    "# Exemplo de uso:\n",
    "X, y = Preprocessing().execute(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Divis√£o dos Dados\n",
    "\n",
    "Os dados s√£o divididos em conjuntos de treino, valida√ß√£o e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Normaliza√ß√£o\n",
    "\n",
    "Os dados s√£o normalizados utilizando Z-score seguido por `MinMaxScaler` para escalar os valores entre 0 e 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X_train: np.ndarray, X_val: np.ndarray, X_test: np.ndarray):\n",
    "    mu = X_train.mean(axis=(0, 1), keepdims=True)\n",
    "    sigma = X_train.std(axis=(0, 1), keepdims=True)\n",
    "    sigma[sigma == 0] = 1.0\n",
    "\n",
    "    X_train_z = (X_train - mu) / sigma\n",
    "    X_val_z   = (X_val - mu) / sigma\n",
    "    X_test_z  = (X_test - mu) / sigma\n",
    "\n",
    "    X_train_final = np.zeros_like(X_train_z)\n",
    "    X_val_final   = np.zeros_like(X_val_z)\n",
    "    X_test_final  = np.zeros_like(X_test_z)\n",
    "\n",
    "    n_channels = X_train_z.shape[2]\n",
    "\n",
    "    for ch in range(n_channels):\n",
    "        vals = X_train_z[:, :, ch].reshape(-1, 1)  # Flatten canal: (N*T, 1)\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaler.fit(vals)  # um √∫nico min/max para todo o canal\n",
    "    \n",
    "        for i in range(X_train_z.shape[0]):  # aplica em cada amostra\n",
    "            X_train_final[i, :, ch] = scaler.transform(X_train_z[i, :, ch].reshape(-1, 1)).flatten()\n",
    "        \n",
    "        for i in range(X_val_z.shape[0]):\n",
    "            X_val_final[i, :, ch]   = scaler.transform(X_val_z[i, :, ch].reshape(-1, 1)).flatten()\n",
    "        \n",
    "        for i in range(X_test_z.shape[0]):\n",
    "            X_test_final[i, :, ch]  = scaler.transform(X_test_z[i, :, ch].reshape(-1, 1)).flatten()\n",
    "\n",
    "    return X_train_final, X_val_final, X_test_final\n",
    "\n",
    "X_train, X_val, X_test = normalize(X_train, X_val, X_test)\n",
    "\n",
    "teste, _, _ = normalize(X, X, X)\n",
    "teste = teste[INDEX]\n",
    "print(teste.shape)\n",
    "df_sample = pd.DataFrame(teste, columns=[f\"canal_{i+1}\" for i in range(teste.shape[1])])\n",
    "df_sample.to_csv(f\"sample_minmax.csv\", index=False)\n",
    "\n",
    "print(f\"Treino: {X_train.shape}, Valida√ß√£o: {X_val.shape}, Teste: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Constru√ß√£o do Modelo\n",
    "\n",
    "Defini√ß√£o da arquitetura do modelo, que consiste em uma rede neural recorrente com camadas LSTM bidirecionais, dropout para regulariza√ß√£o e camadas densas para a classifica√ß√£o final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_model():\n",
    "    N_CHANNELS = 4\n",
    "    N_SAMPLES = 440\n",
    "    N_CLASSES = 10\n",
    "\n",
    "    input = Input(shape=(N_SAMPLES, N_CHANNELS))\n",
    "    x = Bidirectional(LSTM(units=32, return_sequences=True))(input)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Bidirectional(LSTM(units=16, return_sequences=False))(x)\n",
    "    x = Dense(64, activation='elu')(x)\n",
    "    output = Dense(N_CLASSES, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(input, output)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model():\n",
    "    N_CHANNELS = 4\n",
    "    N_SAMPLES = 440\n",
    "    N_CLASSES = 10\n",
    "\n",
    "    input = Input(shape=(N_SAMPLES, N_CHANNELS))\n",
    "    x = Bidirectional(LSTM(units=N_SAMPLES, return_sequences=True))(input)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Bidirectional(LSTM(units=N_SAMPLES // 2, return_sequences=True))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Bidirectional(LSTM(units=N_SAMPLES // 4, return_sequences=False))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='elu')(x)\n",
    "    output = Dense(N_CLASSES, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(input, output)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Treinamento do Modelo\n",
    "\n",
    "Treinamento do modelo com os dados preparados. S√£o utilizados callbacks para `EarlyStopping` (interromper o treino se a performance n√£o melhorar) e `ModelCheckpoint` (salvar o melhor modelo encontrado durante o treino)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'melhor_modelo.keras',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10, \n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop, checkpoint],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "accuracy = history.history[\"accuracy\"]\n",
    "val_accuracy = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy with Preprocessing and Min Max\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss with Preprocessing and Min Max\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Avalia√ß√£o do Modelo\n",
    "\n",
    "Avalia√ß√£o da performance do modelo treinado no conjunto de teste. S√£o calculadas m√©tricas como acur√°cia, precis√£o, recall e F1-score, al√©m da exibi√ß√£o de um relat√≥rio de classifica√ß√£o detalhado por classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, X_test, y_test):\n",
    "    y_pred_probs = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='macro')\n",
    "    rec = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    print(f\"\\nüìä Desempenho no conjunto de teste:\")\n",
    "    print(f\"Acur√°cia: {acc:.4f}\")\n",
    "    print(f\"Precis√£o (macro): {prec:.4f}\")\n",
    "    print(f\"Recall (macro): {rec:.4f}\")\n",
    "    print(f\"F1-score (macro): {f1:.4f}\")\n",
    "\n",
    "    print(\"\\nRelat√≥rio por classe:\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "model = load_model('melhor_modelo.keras')\n",
    "validate(model, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Machine Learning)",
   "language": "python",
   "name": "ml-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
