{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCC - Análise de Sinais EEG\n",
    "\n",
    "Este notebook apresenta um fluxo de trabalho completo para a análise de sinais de EEG, desde a leitura e pré-processamento dos dados até a construção, treinamento e avaliação de um modelo de aprendizado profundo para classificação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bibliotecas e Configurações Iniciais\n",
    "\n",
    "Importação das bibliotecas necessárias e configuração do ambiente, incluindo a alocação de memória da GPU, se disponível."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rclone mount drive-thiago: ~/gdrive --daemon\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, Bidirectional, LSTM, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Configuração da GPU (opcional)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4000)]\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "DATASET = 'MU'\n",
    "N_CLASSES = 10\n",
    "\n",
    "match DATASET:\n",
    "    case 'MU':\n",
    "        N_TIMESTEPS = 440\n",
    "        N_FEATURES = 4\n",
    "\n",
    "    case 'EP':\n",
    "        N_TIMESTEPS = 256\n",
    "        N_FEATURES = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Leitura dos Dados\n",
    "\n",
    "Leitura do arquivo bruto e conversão dos dados em um formato adequado para o treinamento do modelo de deep learning. Cada amostra é convertida em um array NumPy com shape `(n_amostras, TARGET_LEN, n_canais)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv(f'{DATASET}_train.csv')\n",
    "except FileNotFoundError:\n",
    "    df = pd.read_csv(f\"hf://datasets/DavidVivancos/MindBigData2022_MNIST_{DATASET}/train.csv\")\n",
    "    df.to_csv(f'{DATASET}_train.csv', index=False)\n",
    "\n",
    "df = df[df['label'] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 1:].values\n",
    "y = df.iloc[:, 0].values\n",
    "\n",
    "X = X.reshape((-1, N_TIMESTEPS, N_FEATURES))\n",
    "y = y - y.min() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pré-processamento e Filtragem dos Sinais\n",
    "\n",
    "Aplicação de filtros para remover ruídos e artefatos dos sinais de EEG. As seguintes técnicas são utilizadas:\n",
    "- **Filtro Butterworth Passa-Alta:** Para remover a flutuação da linha de base.\n",
    "- **Filtro Notch:** Para remover a interferência da rede elétrica (60 Hz).\n",
    "- **Denoising com Transformada Wavelet Discreta (DWT):** Para atenuar ruídos de alta frequência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import butter, filtfilt, iirnotch\n",
    "import pywt\n",
    "\n",
    "def butterworth_highpass(data, fs=220, cutoff=0.1, order=5):\n",
    "    b, a = butter(order, cutoff / (fs / 2), btype=\"high\", analog=False)\n",
    "    return filtfilt(b, a, data)\n",
    "\n",
    "def notch_filter(data, fs=220, freq=60.0, Q=30.0):\n",
    "    b, a = iirnotch(w0=freq/(fs/2), Q=Q)\n",
    "    return filtfilt(b, a, data)\n",
    "\n",
    "def dwt_denoise_reconstruct(signal, wavelet='db4', level=3, mode='soft'):\n",
    "    coeffs = pywt.wavedec(signal, wavelet=wavelet, level=level)\n",
    "    n = len(signal)\n",
    "    for i in range(1, len(coeffs)):\n",
    "        cd = coeffs[i]\n",
    "        sigma = np.median(np.abs(cd)) / 0.6745 if cd.size > 0 else 0.0\n",
    "        thresh = sigma * np.sqrt(2 * np.log(n)) if sigma > 0 else 0.0\n",
    "        coeffs[i] = pywt.threshold(cd, thresh, mode=mode)\n",
    "    rec = pywt.waverec(coeffs, wavelet=wavelet)\n",
    "    return np.asarray(rec[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filtered = np.zeros_like(X, dtype=float)\n",
    "\n",
    "n_samples, timesteps, n_channels = X.shape\n",
    "\n",
    "for i in range(n_samples):\n",
    "    for ch in range(n_channels):\n",
    "        signal = X[i, :, ch].astype(float)\n",
    "        signal = butterworth_highpass(signal, cutoff=0.1, order=5)\n",
    "        signal = notch_filter(signal, freq=60.0, Q=30.0)\n",
    "        signal = dwt_denoise_reconstruct(signal, wavelet='db4', level=3, mode='soft')\n",
    "        X_filtered[i, :, ch] = signal\n",
    "\n",
    "X = X_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Normalização\n",
    "\n",
    "Os dados são normalizados utilizando Z-score seguido por `MinMaxScaler` para escalar os valores entre 0 e 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_teste(X: np.ndarray):\n",
    "    means = X.mean(axis=0)\n",
    "    stds = X.std(axis=0)\n",
    "    X_zscore = (X - means) / (stds + 1e-8)\n",
    "\n",
    "    scaler = MinMaxScaler() \n",
    "\n",
    "    X = scaler.fit_transform(X_zscore)\n",
    "\n",
    "    return X\n",
    "\n",
    "def normalize(X: np.ndarray):\n",
    "    mu = X.mean(axis=(0, 1), keepdims=True)\n",
    "    sigma = X.std(axis=(0, 1), keepdims=True)\n",
    "    sigma[sigma == 0] = 1.0\n",
    "\n",
    "    X_z = (X - mu) / sigma\n",
    "\n",
    "    X_final = np.zeros_like(X_z)\n",
    "\n",
    "    n_channels = X_z.shape[2]\n",
    "\n",
    "    for ch in range(n_channels):\n",
    "        vals = X_z[:, :, ch].reshape(-1, 1)  # Flatten canal: (N*T, 1)\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaler.fit(vals)  # um único min/max para todo o canal\n",
    "    \n",
    "        for i in range(X_z.shape[0]):  # aplica em cada amostra\n",
    "            X_final[i, :, ch] = scaler.transform(X_z[i, :, ch].reshape(-1, 1)).flatten()\n",
    "        \n",
    "    return X_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = normalize(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Construção do Modelo\n",
    "\n",
    "Definição da arquitetura do modelo, que consiste em uma rede neural recorrente com camadas LSTM bidirecionais, dropout para regularização e camadas densas para a classificação final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_model():\n",
    "\n",
    "    input = Input(shape=(N_TIMESTEPS, N_FEATURES))\n",
    "    x = Bidirectional(LSTM(units=32, return_sequences=True))(input)\n",
    "    x = Bidirectional(LSTM(units=16, return_sequences=False))(x)\n",
    "    x = Dense(64, activation='elu')(x)\n",
    "    output = Dense(N_CLASSES, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(input, output)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model():\n",
    "\n",
    "    input = Input(shape=(N_TIMESTEPS, N_FEATURES))\n",
    "    x = Bidirectional(LSTM(units=N_TIMESTEPS, return_sequences=True))(input)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Bidirectional(LSTM(units=N_TIMESTEPS // 2, return_sequences=True))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Bidirectional(LSTM(units=N_TIMESTEPS // 4, return_sequences=False))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='elu')(x)\n",
    "    output = Dense(N_CLASSES, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(input, output)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.00003125),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Treinamento do Modelo\n",
    "\n",
    "Treinamento do modelo com os dados preparados. São utilizados callbacks para `EarlyStopping` (interromper o treino se a performance não melhorar) e `ModelCheckpoint` (salvar o melhor modelo encontrado durante o treino)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'melhor_modelo.keras',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X, y,\n",
    "    validation_split=0.2,\n",
    "    epochs=10, \n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop, checkpoint],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Criação de Gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "accuracy = history.history[\"accuracy\"]\n",
    "val_accuracy = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, accuracy, \"bo-\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_accuracy, \"b-\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, loss, \"ro-\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"r-\", label=\"Validation loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "PATH = \"(Simple Model) - Gráfico 4 - Preprocessing and Normalization.png\"\n",
    "plt.suptitle(\"Training Results with Preprocessing and Normalization\", fontsize=14)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.savefig(PATH, dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"Figura salva como: {PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Avaliação do Modelo\n",
    "\n",
    "Avaliação da performance do modelo treinado no conjunto de teste. São calculadas métricas como acurácia, precisão, recall e F1-score, além da exibição de um relatório de classificação detalhado por classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, X_test, y_test):\n",
    "    y_pred_probs = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='macro')\n",
    "    rec = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    print(f\"\\n📊 Desempenho no conjunto de teste:\")\n",
    "    print(f\"Acurácia: {acc:.4f}\")\n",
    "    print(f\"Precisão (macro): {prec:.4f}\")\n",
    "    print(f\"Recall (macro): {rec:.4f}\")\n",
    "    print(f\"F1-score (macro): {f1:.4f}\")\n",
    "\n",
    "    print(\"\\nRelatório por classe:\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"hf://datasets/DavidVivancos/MindBigData2022_MNIST_{DATASET}/test.csv\")\n",
    "\n",
    "df = df[df['label'] != -1]\n",
    "\n",
    "X_test = df.iloc[:, 1:].values\n",
    "y_test = df.iloc[:, 0].values\n",
    "\n",
    "model = load_model('melhor_modelo.keras')\n",
    "validate(model, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Machine Learning)",
   "language": "python",
   "name": "ml-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
