{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCC - Análise de Sinais EEG\n",
    "\n",
    "Este notebook apresenta um fluxo de trabalho completo para a análise de sinais de EEG, desde a leitura e pré-processamento dos dados até a construção, treinamento e avaliação de um modelo de aprendizado profundo para classificação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bibliotecas e Configurações Iniciais\n",
    "\n",
    "Importação das bibliotecas necessárias e configuração do ambiente, incluindo a alocação de memória da GPU, se disponível."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pywt\n",
    "from scipy.signal import butter, filtfilt, iirnotch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Bidirectional, LSTM, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Configuração da GPU (opcional)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4000)]\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Caminhos dos arquivos\n",
    "DATALAKE_DIR = '/home/thiago/gdrive/UFRGS/TCC/tcc-ufrgs/datalake'\n",
    "RAW_MUSE_PATH = f'{DATALAKE_DIR}/raw/Muse-v1.0/MU.txt'\n",
    "CSV_MUSE_PATH = f'{DATALAKE_DIR}/raw/Muse-v1.0/MU.csv'\n",
    "PREPROCESSED_MUSE_PATH = f'{DATALAKE_DIR}/processed/Muse-v1.0/MU_filtered.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Leitura e Preparação dos Dados\n",
    "\n",
    "Esta seção é responsável por carregar os dados brutos, convertê-los para um formato CSV mais estruturado e, em seguida, realizar a leitura para o pré-processamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_muse_v1_csv(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Converte o arquivo de dados brutos (formato .txt) para um arquivo CSV.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_path):\n",
    "        col_names = [\"id\", \"event\", \"device\", \"channel\", \"code\", \"size\", \"data\"]\n",
    "        df = pd.read_csv(input_path, header=None, sep='\\t', names=col_names)\n",
    "        df.to_csv(output_path, index=False, sep=';')\n",
    "        print(f\"Arquivo CSV gerado em: {output_path}\")\n",
    "    else:\n",
    "        print(f\"Arquivo CSV já existe em: {output_path}\")\n",
    "\n",
    "# Gerar o CSV a partir do arquivo .txt\n",
    "generate_muse_v1_csv(RAW_MUSE_PATH, CSV_MUSE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pré-processamento e Filtragem dos Sinais\n",
    "\n",
    "Aplicação de filtros para remover ruídos e artefatos dos sinais de EEG. As seguintes técnicas são utilizadas:\n",
    "- **Filtro Butterworth Passa-Alta:** Para remover a flutuação da linha de base.\n",
    "- **Filtro Notch:** Para remover a interferência da rede elétrica (60 Hz).\n",
    "- **Denoising com Transformada Wavelet Discreta (DWT):** Para atenuar ruídos de alta frequência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def butterworth_highpass(self, data, cutoff, fs, order):\n",
    "        b, a = butter(order, cutoff / (fs / 2), btype=\"high\", analog=False)\n",
    "        return filtfilt(b, a, data)\n",
    "\n",
    "    def notch_filter(self, data, fs, freq, Q):\n",
    "        b, a = iirnotch(w0=freq/(fs/2), Q=Q)\n",
    "        return filtfilt(b, a, data)\n",
    "\n",
    "    def dwt_denoise_reconstruct(self, signal, wavelet='db4', level=3, mode='soft'):\n",
    "        coeffs = pywt.wavedec(signal, wavelet=wavelet, level=level)\n",
    "        n = len(signal)\n",
    "        for i in range(1, len(coeffs)):\n",
    "            cd = coeffs[i]\n",
    "            sigma = np.median(np.abs(cd)) / 0.6745 if cd.size > 0 else 0.0\n",
    "            thresh = sigma * np.sqrt(2 * np.log(n)) if sigma > 0 else 0.0\n",
    "            coeffs[i] = pywt.threshold(cd, thresh, mode=mode)\n",
    "        rec = pywt.waverec(coeffs, wavelet=wavelet)\n",
    "        return np.asarray(rec[:n])\n",
    "\n",
    "    def read_input(self, input_path:str) -> pd.DataFrame:\n",
    "        if not os.path.exists(input_path):\n",
    "            raise FileNotFoundError(f\"Arquivo não encontrado: {input_path}\")\n",
    "        return pd.read_csv(input_path, sep=';')\n",
    "\n",
    "    def execute(self, input_path, output_path):\n",
    "        df = self.read_input(input_path)\n",
    "        df = df[df['code'] != -1]\n",
    "        df_other = df[[\"event\", \"device\", \"code\", \"size\"]].drop_duplicates(subset=[\"event\"])\n",
    "        df_pivot = df.pivot(index=\"event\", columns=\"channel\", values=\"data\").reset_index()\n",
    "        CHANNELS = [col for col in df_pivot.columns if col not in [\"event\"]]\n",
    "        df_pivot = df_pivot.merge(df_other, on=\"event\", how=\"inner\")\n",
    "        fs = 220\n",
    "        filtered = []\n",
    "        for _, row in df_pivot.iterrows():\n",
    "            filtered_row = {\n",
    "                'event': row['event'],\n",
    "                'device': row['device'],\n",
    "                'code': row['code'],\n",
    "                'size': row['size']\n",
    "            }\n",
    "            for channel in CHANNELS:\n",
    "                data = np.array([int(x) for x in row[channel].split(',')], dtype=float)\n",
    "                data = self.butterworth_highpass(data=data, cutoff=0.1, fs=fs, order=5)\n",
    "                data = self.notch_filter(data=data, fs=fs, freq=60.0, Q=30.0)\n",
    "                data = self.dwt_denoise_reconstruct(signal=data, wavelet='db4', level=3, mode='soft')\n",
    "                filtered_row[channel] = ','.join(map(lambda v: f\"{v:.6f}\", data))\n",
    "            filtered.append(filtered_row)\n",
    "        df_filtered = pd.DataFrame(filtered)\n",
    "        df_filtered.to_csv(output_path, index=False, sep=';')\n",
    "        print(\"Filtragem + DWT concluídas e CSV salvo em:\", output_path)\n",
    "\n",
    "if not os.path.exists(PREPROCESSED_MUSE_PATH):\n",
    "    Preprocessing().execute(\n",
    "        input_path=CSV_MUSE_PATH, \n",
    "        output_path=PREPROCESSED_MUSE_PATH\n",
    "    )\n",
    "else:\n",
    "    print(f\"Arquivo pré-processado já existe em: {PREPROCESSED_MUSE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Carregamento dos Dados para o Modelo\n",
    "\n",
    "Leitura do arquivo CSV pré-processado e transformação dos dados em um formato adequado para o treinamento do modelo de deep learning. Cada amostra é convertida em um array NumPy com shape `(n_amostras, TARGET_LEN, n_canais)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ler_csv(file_path):\n",
    "    CHANNELS = ['FP1', 'FP2', 'TP10', 'TP9']\n",
    "    TARGET_LEN = 440\n",
    "\n",
    "    df = pd.read_csv(file_path, sep=';')\n",
    "    df = df[df['code'] != -1]\n",
    "\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        channels_data = []\n",
    "        for ch in CHANNELS:\n",
    "            arr = np.array([float(x) for x in row[ch].split(',')])\n",
    "            if len(arr) > TARGET_LEN:\n",
    "                arr = arr[:TARGET_LEN]\n",
    "            elif len(arr) < TARGET_LEN:\n",
    "                arr = np.pad(arr, (0, TARGET_LEN - len(arr)), mode='constant')\n",
    "            channels_data.append(arr)\n",
    "\n",
    "        sample = np.stack(channels_data, axis=1)\n",
    "        X_list.append(sample)\n",
    "        y_list.append(int(row['code']))\n",
    "\n",
    "    X = np.stack(X_list, axis=0)\n",
    "    y = np.array(y_list, dtype=int)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def load_data():\n",
    "    df = pd.read_csv(RAW_MUSE_PATH, sep='\\t', header=None,\n",
    "                     names=[\"id\", \"event\", \"device\", \"channel\", \"code\", \"size\", \"data\"])\n",
    "\n",
    "    df = df[df['code'] != -1]\n",
    "\n",
    "    # Converter 'data' para array NumPy\n",
    "    df[\"data_array\"] = df[\"data\"].apply(lambda x: np.array([int(v) for v in x.split(\",\")]))\n",
    "    \n",
    "    # Ordenar para consistência\n",
    "    df = df.sort_values([\"event\", \"channel\"]).reset_index(drop=True)\n",
    "\n",
    "    # Truncar para o menor tamanho\n",
    "    df[\"len\"] = df[\"data_array\"].apply(len)\n",
    "    min_len = df[\"len\"].min()\n",
    "    print(f\"Menor tamanho de sequência encontrado: {min_len}\")\n",
    "    df[\"data_array\"] = df[\"data_array\"].apply(lambda x: x[:min_len])\n",
    "\n",
    "    # Agrupar por evento\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for event, group in df.groupby(\"event\"):\n",
    "        channel_arrays = [arr for arr in group[\"data_array\"].to_list()]\n",
    "        sample = np.stack(channel_arrays, axis=-1)  # (timesteps, n_channels)\n",
    "        X.append(sample)\n",
    "        y.append(group[\"code\"].iloc[0])\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    print(\"Shape final de X:\", X.shape)\n",
    "    print(\"Shape final de y:\", y.shape)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X, y = load_data()\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# # Normalização MinMax por canal usando apenas dados de treino\n",
    "# X_train_norm = X_train.copy()\n",
    "# X_val_norm = X_val.copy()\n",
    "# X_test_norm = X_test.copy()\n",
    "\n",
    "# n_channels = X_train.shape[2]\n",
    "\n",
    "# for ch in range(n_channels):\n",
    "#     ch_train = X_train[:, :, ch]\n",
    "#     min_val = ch_train.min()\n",
    "#     max_val = ch_train.max()\n",
    "#     # evitar divisão por zero\n",
    "#     if max_val - min_val == 0:\n",
    "#         max_val += 1e-8\n",
    "\n",
    "#     # Normalizar treino\n",
    "#     X_train_norm[:, :, ch] = (X_train[:, :, ch] - min_val) / (max_val - min_val)\n",
    "#     # Aplicar mesma normalização a val e test\n",
    "#     X_val_norm[:, :, ch] = (X_val[:, :, ch] - min_val) / (max_val - min_val)\n",
    "#     X_test_norm[:, :, ch] = (X_test[:, :, ch] - min_val) / (max_val - min_val)\n",
    "\n",
    "# print(\"Normalização MinMax por canal aplicada usando apenas dados de treino.\")\n",
    "\n",
    "# # Substituir arrays originais por normalizados\n",
    "# X_train, X_val, X_test = X_train_norm, X_val_norm, X_test_norm\n",
    "\n",
    "# print(\"Shapes finais:\")\n",
    "# print(\"X_train:\", X_train.shape, \"y_train:\", y_train.shape)\n",
    "# print(\"X_val:\", X_val.shape, \"y_val:\", y_val.shape)\n",
    "# print(\"X_test:\", X_test.shape, \"y_test:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Normalização e Divisão dos Dados\n",
    "\n",
    "Normalização dos dados utilizando Z-score seguido por `MinMaxScaler` para escalar os valores entre 0 e 1. Em seguida, os dados são divididos em conjuntos de treino, validação e teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X_train: np.ndarray, X_val: np.ndarray, X_test: np.ndarray):\n",
    "    mu = X_train.mean(axis=(0, 1), keepdims=True)\n",
    "    sigma = X_train.std(axis=(0, 1), keepdims=True)\n",
    "    sigma[sigma == 0] = 1.0\n",
    "\n",
    "    X_train_z = (X_train - mu) / sigma\n",
    "    X_val_z   = (X_val - mu) / sigma\n",
    "    X_test_z  = (X_test - mu) / sigma\n",
    "\n",
    "    X_train_final = np.zeros_like(X_train_z)\n",
    "    X_val_final   = np.zeros_like(X_val_z)\n",
    "    X_test_final  = np.zeros_like(X_test_z)\n",
    "\n",
    "    n_channels = X_train.shape[2]\n",
    "\n",
    "    for ch in range(n_channels):\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        X_train_final[:, :, ch] = scaler.fit_transform(X_train_z[:, :, ch])\n",
    "        X_val_final[:, :, ch]   = scaler.transform(X_val_z[:, :, ch])\n",
    "        X_test_final[:, :, ch]  = scaler.transform(X_test_z[:, :, ch])\n",
    "\n",
    "    return X_train_final, X_val_final, X_test_final\n",
    "\n",
    "# Divisão dos dados\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Normalização\n",
    "X_train, X_val, X_test = normalize(X_train, X_val, X_test)\n",
    "\n",
    "print(f\"Treino: {X_train.shape}, Validação: {X_val.shape}, Teste: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Construção do Modelo\n",
    "\n",
    "Definição da arquitetura do modelo, que consiste em uma rede neural recorrente com camadas LSTM bidirecionais, dropout para regularização e camadas densas para a classificação final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    N_CHANNELS = 4\n",
    "    N_SAMPLES = 408\n",
    "    N_CLASSES = 10\n",
    "\n",
    "    # model = Sequential([\n",
    "    #     Input(shape=(N_SAMPLES, 4)),\n",
    "    #     Bidirectional(LSTM(units=N_SAMPLES, return_sequences=True)),\n",
    "    #     Dropout(0.1),\n",
    "    #     Bidirectional(LSTM(units=N_SAMPLES // 2, return_sequences=True)),\n",
    "    #     Dropout(0.1),\n",
    "    #     Bidirectional(LSTM(units=N_SAMPLES // 4)),\n",
    "    #     Dropout(0.1),\n",
    "    #     Dense(128, activation='elu'),\n",
    "    #     Dense(N_CLASSES, activation='softmax')\n",
    "    # ])\n",
    "\n",
    "    input = Input(shape=(N_SAMPLES, N_CHANNELS))\n",
    "    x = Bidirectional(LSTM(units=N_SAMPLES, return_sequences=True))(input)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Bidirectional(LSTM(units=N_SAMPLES // 2, return_sequences=True))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Bidirectional(LSTM(units=N_SAMPLES // 4, return_sequences=False))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='elu')(x)\n",
    "    output = Dense(N_CLASSES, activation='softmax', name='output_softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(input, output)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Treinamento do Modelo\n",
    "\n",
    "Treinamento do modelo com os dados preparados. São utilizados callbacks para `EarlyStopping` (interromper o treino se a performance não melhorar) e `ModelCheckpoint` (salvar o melhor modelo encontrado durante o treino)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'melhor_modelo.keras',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10, \n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop, checkpoint],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Avaliação do Modelo\n",
    "\n",
    "Avaliação da performance do modelo treinado no conjunto de teste. São calculadas métricas como acurácia, precisão, recall e F1-score, além da exibição de um relatório de classificação detalhado por classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, X_test, y_test):\n",
    "    y_pred_probs = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='macro')\n",
    "    rec = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    print(f\"\\n📊 Desempenho no conjunto de teste:\")\n",
    "    print(f\"Acurácia: {acc:.4f}\")\n",
    "    print(f\"Precisão (macro): {prec:.4f}\")\n",
    "    print(f\"Recall (macro): {rec:.4f}\")\n",
    "    print(f\"F1-score (macro): {f1:.4f}\")\n",
    "\n",
    "    print(\"\\nRelatório por classe:\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "validate(model, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Machine Learning)",
   "language": "python",
   "name": "ml-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
